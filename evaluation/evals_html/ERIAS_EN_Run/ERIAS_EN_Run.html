<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - ERIAS_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>ERIAS_EN_Run.1.dat</h4>
            <pre>
runid                 	all	10
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	2537
map                   	all	0.3111
gm_map                	all	0.2502
Rprec                 	all	0.3674
bpref                 	all	0.4743
recip_rank            	all	0.5639
iprec_at_recall_0.00  	all	0.7091
iprec_at_recall_0.10  	all	0.6293
iprec_at_recall_0.20  	all	0.5730
iprec_at_recall_0.30  	all	0.4590
iprec_at_recall_0.40  	all	0.3714
iprec_at_recall_0.50  	all	0.3154
iprec_at_recall_0.60  	all	0.2578
iprec_at_recall_0.70  	all	0.1802
iprec_at_recall_0.80  	all	0.1143
iprec_at_recall_0.90  	all	0.0655
iprec_at_recall_1.00  	all	0.0194
P_5                   	all	0.5040
P_10                  	all	0.5080
P_15                  	all	0.5000
P_20                  	all	0.4840
P_30                  	all	0.4513
P_100                 	all	0.2994
P_200                 	all	0.1893
P_500                 	all	0.0919
P_1000                	all	0.0507
</pre>
 <h4>ERIAS_EN_Run.5.dat</h4>
            <pre>
runid                 	all	10
num_q                 	all	50
num_ret               	all	49145
num_rel               	all	3209
num_rel_ret           	all	2061
map                   	all	0.2217
gm_map                	all	0.1253
Rprec                 	all	0.2510
bpref                 	all	0.4016
recip_rank            	all	0.6795
iprec_at_recall_0.00  	all	0.7444
iprec_at_recall_0.10  	all	0.5441
iprec_at_recall_0.20  	all	0.3814
iprec_at_recall_0.30  	all	0.2982
iprec_at_recall_0.40  	all	0.2322
iprec_at_recall_0.50  	all	0.1861
iprec_at_recall_0.60  	all	0.1514
iprec_at_recall_0.70  	all	0.0989
iprec_at_recall_0.80  	all	0.0680
iprec_at_recall_0.90  	all	0.0362
iprec_at_recall_1.00  	all	0.0140
P_5                   	all	0.5440
P_10                  	all	0.5280
P_15                  	all	0.4427
P_20                  	all	0.3970
P_30                  	all	0.3427
P_100                 	all	0.1950
P_200                 	all	0.1392
P_500                 	all	0.0714
P_1000                	all	0.0412
</pre>
 <h4>ERIAS_EN_Run.6.dat</h4>
            <pre>
runid                 	all	10
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	2148
map                   	all	0.2315
gm_map                	all	0.1435
Rprec                 	all	0.2572
bpref                 	all	0.4212
recip_rank            	all	0.7084
iprec_at_recall_0.00  	all	0.7731
iprec_at_recall_0.10  	all	0.5392
iprec_at_recall_0.20  	all	0.4042
iprec_at_recall_0.30  	all	0.3288
iprec_at_recall_0.40  	all	0.2475
iprec_at_recall_0.50  	all	0.1948
iprec_at_recall_0.60  	all	0.1573
iprec_at_recall_0.70  	all	0.1104
iprec_at_recall_0.80  	all	0.0658
iprec_at_recall_0.90  	all	0.0371
iprec_at_recall_1.00  	all	0.0118
P_5                   	all	0.5720
P_10                  	all	0.5460
P_15                  	all	0.4640
P_20                  	all	0.4220
P_30                  	all	0.3627
P_100                 	all	0.2084
P_200                 	all	0.1451
P_500                 	all	0.0752
P_1000                	all	0.0430
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>ERIAS_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4955
ndcg_cut_10           	all	0.5023
ndcg_cut_15           	all	0.5029
ndcg_cut_20           	all	0.4963
ndcg_cut_30           	all	0.4836
ndcg_cut_100          	all	0.4860
ndcg_cut_200          	all	0.5470
ndcg_cut_500          	all	0.6056
ndcg_cut_1000         	all	0.6344
</pre>
 <h4>ERIAS_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.5470
ndcg_cut_10           	all	0.5376
ndcg_cut_15           	all	0.4813
ndcg_cut_20           	all	0.4468
ndcg_cut_30           	all	0.4066
ndcg_cut_100          	all	0.3602
ndcg_cut_200          	all	0.4277
ndcg_cut_500          	all	0.4852
ndcg_cut_1000         	all	0.5229
</pre>
 <h4>ERIAS_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.5702
ndcg_cut_10           	all	0.5574
ndcg_cut_15           	all	0.5027
ndcg_cut_20           	all	0.4716
ndcg_cut_30           	all	0.4272
ndcg_cut_100          	all	0.3765
ndcg_cut_200          	all	0.4428
ndcg_cut_500          	all	0.5061
ndcg_cut_1000         	all	0.5447
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>ERIAS_EN_Run.1.dat</h4>
<img src="./img/ERIAS_EN_Run.1.dat.p10.png"/>
 <h4>ERIAS_EN_Run.5.dat</h4>
<img src="./img/ERIAS_EN_Run.5.dat.p10.png"/>
 <h4>ERIAS_EN_Run.6.dat</h4>
<img src="./img/ERIAS_EN_Run.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
