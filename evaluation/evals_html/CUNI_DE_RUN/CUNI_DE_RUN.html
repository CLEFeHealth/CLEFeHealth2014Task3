<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - CUNI_DE_RUN</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>CUNI_DE_RUN.1.dat</h4>
            <pre>
runid                 	all	RUN1
num_q                 	all	50
num_ret               	all	47542
num_rel               	all	3172
num_rel_ret           	all	1806
map                   	all	0.1834
gm_map                	all	0.0348
Rprec                 	all	0.2283
bpref                 	all	0.3359
recip_rank            	all	0.4778
iprec_at_recall_0.00  	all	0.5851
iprec_at_recall_0.10  	all	0.4077
iprec_at_recall_0.20  	all	0.3159
iprec_at_recall_0.30  	all	0.2701
iprec_at_recall_0.40  	all	0.2318
iprec_at_recall_0.50  	all	0.1904
iprec_at_recall_0.60  	all	0.1351
iprec_at_recall_0.70  	all	0.0990
iprec_at_recall_0.80  	all	0.0480
iprec_at_recall_0.90  	all	0.0241
iprec_at_recall_1.00  	all	0.0056
P_5                   	all	0.3760
P_10                  	all	0.3920
P_15                  	all	0.3507
P_20                  	all	0.3270
P_30                  	all	0.2940
P_100                 	all	0.1878
P_200                 	all	0.1240
P_500                 	all	0.0634
P_1000                	all	0.0361
</pre>
 <h4>CUNI_DE_RUN.5.dat</h4>
            <pre>
runid                 	all	RUN5
num_q                 	all	50
num_ret               	all	48062
num_rel               	all	3209
num_rel_ret           	all	1935
map                   	all	0.2014
gm_map                	all	0.0721
Rprec                 	all	0.2463
bpref                 	all	0.3629
recip_rank            	all	0.5192
iprec_at_recall_0.00  	all	0.6281
iprec_at_recall_0.10  	all	0.4527
iprec_at_recall_0.20  	all	0.3592
iprec_at_recall_0.30  	all	0.2903
iprec_at_recall_0.40  	all	0.2499
iprec_at_recall_0.50  	all	0.2041
iprec_at_recall_0.60  	all	0.1469
iprec_at_recall_0.70  	all	0.1029
iprec_at_recall_0.80  	all	0.0480
iprec_at_recall_0.90  	all	0.0241
iprec_at_recall_1.00  	all	0.0056
P_5                   	all	0.4160
P_10                  	all	0.4280
P_15                  	all	0.3827
P_20                  	all	0.3600
P_30                  	all	0.3160
P_100                 	all	0.2020
P_200                 	all	0.1334
P_500                 	all	0.0680
P_1000                	all	0.0387
</pre>
 <h4>CUNI_DE_RUN.6.dat</h4>
            <pre>
runid                 	all	RUN6
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	1517
map                   	all	0.1348
gm_map                	all	0.0192
Rprec                 	all	0.1671
bpref                 	all	0.3054
recip_rank            	all	0.5405
iprec_at_recall_0.00  	all	0.5820
iprec_at_recall_0.10  	all	0.3289
iprec_at_recall_0.20  	all	0.2636
iprec_at_recall_0.30  	all	0.1905
iprec_at_recall_0.40  	all	0.1272
iprec_at_recall_0.50  	all	0.0978
iprec_at_recall_0.60  	all	0.0779
iprec_at_recall_0.70  	all	0.0530
iprec_at_recall_0.80  	all	0.0242
iprec_at_recall_0.90  	all	0.0107
iprec_at_recall_1.00  	all	0.0035
P_5                   	all	0.3880
P_10                  	all	0.3820
P_15                  	all	0.3147
P_20                  	all	0.2700
P_30                  	all	0.2400
P_100                 	all	0.1298
P_200                 	all	0.0857
P_500                 	all	0.0492
P_1000                	all	0.0303
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>CUNI_DE_RUN.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3561
ndcg_cut_10           	all	0.3681
ndcg_cut_15           	all	0.3473
ndcg_cut_20           	all	0.3309
ndcg_cut_30           	all	0.3137
ndcg_cut_100          	all	0.3086
ndcg_cut_200          	all	0.3546
ndcg_cut_500          	all	0.4047
ndcg_cut_1000         	all	0.4322
</pre>
 <h4>CUNI_DE_RUN.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.3963
ndcg_cut_10           	all	0.4058
ndcg_cut_15           	all	0.3816
ndcg_cut_20           	all	0.3651
ndcg_cut_30           	all	0.3404
ndcg_cut_100          	all	0.3387
ndcg_cut_200          	all	0.3886
ndcg_cut_500          	all	0.4425
ndcg_cut_1000         	all	0.4715
</pre>
 <h4>CUNI_DE_RUN.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4125
ndcg_cut_10           	all	0.4024
ndcg_cut_15           	all	0.3523
ndcg_cut_20           	all	0.3174
ndcg_cut_30           	all	0.2882
ndcg_cut_100          	all	0.2426
ndcg_cut_200          	all	0.2722
ndcg_cut_500          	all	0.3259
ndcg_cut_1000         	all	0.3626
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>CUNI_DE_RUN.1.dat</h4>
<img src="./img/CUNI_DE_RUN.1.dat.p10.png"/>
 <h4>CUNI_DE_RUN.5.dat</h4>
<img src="./img/CUNI_DE_RUN.5.dat.p10.png"/>
 <h4>CUNI_DE_RUN.6.dat</h4>
<img src="./img/CUNI_DE_RUN.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
