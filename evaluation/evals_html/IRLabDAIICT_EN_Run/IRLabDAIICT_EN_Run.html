<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - IRLabDAIICT_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>IRLabDAIICT_EN_Run.1.dat</h4>
            <pre>
runid                 	all	1
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2503
map                   	all	0.4096
gm_map                	all	0.3193
Rprec                 	all	0.4285
bpref                 	all	0.5711
recip_rank            	all	0.8289
iprec_at_recall_0.00  	all	0.8888
iprec_at_recall_0.10  	all	0.7800
iprec_at_recall_0.20  	all	0.7256
iprec_at_recall_0.30  	all	0.6420
iprec_at_recall_0.40  	all	0.5284
iprec_at_recall_0.50  	all	0.3992
iprec_at_recall_0.60  	all	0.3085
iprec_at_recall_0.70  	all	0.2143
iprec_at_recall_0.80  	all	0.1282
iprec_at_recall_0.90  	all	0.0744
iprec_at_recall_1.00  	all	0.0209
P_5                   	all	0.7120
P_10                  	all	0.7060
P_15                  	all	0.6800
P_20                  	all	0.6480
P_30                  	all	0.5787
P_100                 	all	0.3162
P_200                 	all	0.1923
P_500                 	all	0.0919
P_1000                	all	0.0501
</pre>
 <h4>IRLabDAIICT_EN_Run.2.dat</h4>
            <pre>
runid                 	all	ds1
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2558
map                   	all	0.4146
gm_map                	all	0.3357
Rprec                 	all	0.4415
bpref                 	all	0.5874
recip_rank            	all	0.8208
iprec_at_recall_0.00  	all	0.8879
iprec_at_recall_0.10  	all	0.7936
iprec_at_recall_0.20  	all	0.7327
iprec_at_recall_0.30  	all	0.6422
iprec_at_recall_0.40  	all	0.5491
iprec_at_recall_0.50  	all	0.4329
iprec_at_recall_0.60  	all	0.3199
iprec_at_recall_0.70  	all	0.2136
iprec_at_recall_0.80  	all	0.1331
iprec_at_recall_0.90  	all	0.0673
iprec_at_recall_1.00  	all	0.0186
P_5                   	all	0.7040
P_10                  	all	0.7020
P_15                  	all	0.6907
P_20                  	all	0.6450
P_30                  	all	0.5753
P_100                 	all	0.3252
P_200                 	all	0.1964
P_500                 	all	0.0940
P_1000                	all	0.0512
</pre>
 <h4>IRLabDAIICT_EN_Run.3.dat</h4>
            <pre>
runid                 	all	ds2okp
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2032
map                   	all	0.2507
gm_map                	all	0.1257
Rprec                 	all	0.2807
bpref                 	all	0.4490
recip_rank            	all	0.6461
iprec_at_recall_0.00  	all	0.7271
iprec_at_recall_0.10  	all	0.5589
iprec_at_recall_0.20  	all	0.4410
iprec_at_recall_0.30  	all	0.3562
iprec_at_recall_0.40  	all	0.3014
iprec_at_recall_0.50  	all	0.2175
iprec_at_recall_0.60  	all	0.1585
iprec_at_recall_0.70  	all	0.1143
iprec_at_recall_0.80  	all	0.0849
iprec_at_recall_0.90  	all	0.0487
iprec_at_recall_1.00  	all	0.0119
P_5                   	all	0.5480
P_10                  	all	0.5640
P_15                  	all	0.4973
P_20                  	all	0.4490
P_30                  	all	0.3773
P_100                 	all	0.2108
P_200                 	all	0.1346
P_500                 	all	0.0693
P_1000                	all	0.0406
</pre>
 <h4>IRLabDAIICT_EN_Run.5.dat</h4>
            <pre>
runid                 	all	2
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2250
map                   	all	0.3026
gm_map                	all	0.1839
Rprec                 	all	0.3202
bpref                 	all	0.5041
recip_rank            	all	0.7316
iprec_at_recall_0.00  	all	0.7943
iprec_at_recall_0.10  	all	0.6610
iprec_at_recall_0.20  	all	0.5341
iprec_at_recall_0.30  	all	0.4479
iprec_at_recall_0.40  	all	0.3440
iprec_at_recall_0.50  	all	0.2606
iprec_at_recall_0.60  	all	0.1968
iprec_at_recall_0.70  	all	0.1336
iprec_at_recall_0.80  	all	0.0987
iprec_at_recall_0.90  	all	0.0590
iprec_at_recall_1.00  	all	0.0137
P_5                   	all	0.6680
P_10                  	all	0.6540
P_15                  	all	0.5573
P_20                  	all	0.4910
P_30                  	all	0.4207
P_100                 	all	0.2466
P_200                 	all	0.1527
P_500                 	all	0.0788
P_1000                	all	0.0450
</pre>
 <h4>IRLabDAIICT_EN_Run.6.txt</h4>
            <pre>
runid                 	all	3
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2529
map                   	all	0.3686
gm_map                	all	0.2865
Rprec                 	all	0.4009
bpref                 	all	0.5662
recip_rank            	all	0.8188
iprec_at_recall_0.00  	all	0.8649
iprec_at_recall_0.10  	all	0.7406
iprec_at_recall_0.20  	all	0.6432
iprec_at_recall_0.30  	all	0.5549
iprec_at_recall_0.40  	all	0.4642
iprec_at_recall_0.50  	all	0.3605
iprec_at_recall_0.60  	all	0.2633
iprec_at_recall_0.70  	all	0.2023
iprec_at_recall_0.80  	all	0.1158
iprec_at_recall_0.90  	all	0.0504
iprec_at_recall_1.00  	all	0.0122
P_5                   	all	0.7320
P_10                  	all	0.6880
P_15                  	all	0.6240
P_20                  	all	0.5810
P_30                  	all	0.5100
P_100                 	all	0.3024
P_200                 	all	0.1893
P_500                 	all	0.0928
P_1000                	all	0.0506
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>IRLabDAIICT_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6926
ndcg_cut_10           	all	0.6869
ndcg_cut_15           	all	0.6776
ndcg_cut_20           	all	0.6651
ndcg_cut_30           	all	0.6268
ndcg_cut_100          	all	0.5556
ndcg_cut_200          	all	0.6039
ndcg_cut_500          	all	0.6586
ndcg_cut_1000         	all	0.6842
</pre>
 <h4>IRLabDAIICT_EN_Run.2.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6862
ndcg_cut_10           	all	0.6889
ndcg_cut_15           	all	0.6850
ndcg_cut_20           	all	0.6629
ndcg_cut_30           	all	0.6268
ndcg_cut_100          	all	0.5635
ndcg_cut_200          	all	0.6130
ndcg_cut_500          	all	0.6687
ndcg_cut_1000         	all	0.6945
</pre>
 <h4>IRLabDAIICT_EN_Run.3.dat</h4>
            <pre>
ndcg_cut_5            	all	0.5582
ndcg_cut_10           	all	0.5658
ndcg_cut_15           	all	0.5262
ndcg_cut_20           	all	0.4918
ndcg_cut_30           	all	0.4435
ndcg_cut_100          	all	0.3934
ndcg_cut_200          	all	0.4406
ndcg_cut_500          	all	0.4951
ndcg_cut_1000         	all	0.5310
</pre>
 <h4>IRLabDAIICT_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6523
ndcg_cut_10           	all	0.6363
ndcg_cut_15           	all	0.5815
ndcg_cut_20           	all	0.5402
ndcg_cut_30           	all	0.4953
ndcg_cut_100          	all	0.4540
ndcg_cut_200          	all	0.5024
ndcg_cut_500          	all	0.5669
ndcg_cut_1000         	all	0.6013
</pre>
 <h4>IRLabDAIICT_EN_Run.6.txt</h4>
            <pre>
ndcg_cut_5            	all	0.7174
ndcg_cut_10           	all	0.6875
ndcg_cut_15           	all	0.6468
ndcg_cut_20           	all	0.6184
ndcg_cut_30           	all	0.5741
ndcg_cut_100          	all	0.5245
ndcg_cut_200          	all	0.5828
ndcg_cut_500          	all	0.6448
ndcg_cut_1000         	all	0.6706
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>IRLabDAIICT_EN_Run.1.dat</h4>
<img src="./img/IRLabDAIICT_EN_Run.1.dat.p10.png"/>
 <h4>IRLabDAIICT_EN_Run.2.dat</h4>
<img src="./img/IRLabDAIICT_EN_Run.2.dat.p10.png"/>
 <h4>IRLabDAIICT_EN_Run.3.dat</h4>
<img src="./img/IRLabDAIICT_EN_Run.3.dat.p10.png"/>
 <h4>IRLabDAIICT_EN_Run.5.dat</h4>
<img src="./img/IRLabDAIICT_EN_Run.5.dat.p10.png"/>
 <h4>IRLabDAIICT_EN_Run.6.txt</h4>
<img src="./img/IRLabDAIICT_EN_Run.6.txt.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
