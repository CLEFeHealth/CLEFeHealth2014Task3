<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - SNUMEDINFO_CZ_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>SNUMEDINFO_CZ_Run.1.dat</h4>
            <pre>
runid                 	all	SNUMedinfo1
num_q                 	all	50
num_ret               	all	49000
num_rel               	all	3195
num_rel_ret           	all	2147
map                   	all	0.3404
gm_map                	all	0.1934
Rprec                 	all	0.3573
bpref                 	all	0.4956
recip_rank            	all	0.8074
iprec_at_recall_0.00  	all	0.8804
iprec_at_recall_0.10  	all	0.8032
iprec_at_recall_0.20  	all	0.6430
iprec_at_recall_0.30  	all	0.4941
iprec_at_recall_0.40  	all	0.3969
iprec_at_recall_0.50  	all	0.2791
iprec_at_recall_0.60  	all	0.2030
iprec_at_recall_0.70  	all	0.1307
iprec_at_recall_0.80  	all	0.0919
iprec_at_recall_0.90  	all	0.0465
iprec_at_recall_1.00  	all	0.0155
P_5                   	all	0.7680
P_10                  	all	0.7220
P_15                  	all	0.6773
P_20                  	all	0.6090
P_30                  	all	0.5127
P_100                 	all	0.2742
P_200                 	all	0.1632
P_500                 	all	0.0772
P_1000                	all	0.0429
</pre>
 <h4>SNUMEDINFO_CZ_Run.5.dat</h4>
            <pre>
runid                 	all	SNUMEDINFO5
num_q                 	all	50
num_ret               	all	49000
num_rel               	all	3195
num_rel_ret           	all	2147
map                   	all	0.3424
gm_map                	all	0.1901
Rprec                 	all	0.3677
bpref                 	all	0.4910
recip_rank            	all	0.7991
iprec_at_recall_0.00  	all	0.8529
iprec_at_recall_0.10  	all	0.7850
iprec_at_recall_0.20  	all	0.6380
iprec_at_recall_0.30  	all	0.4787
iprec_at_recall_0.40  	all	0.3906
iprec_at_recall_0.50  	all	0.3045
iprec_at_recall_0.60  	all	0.2149
iprec_at_recall_0.70  	all	0.1464
iprec_at_recall_0.80  	all	0.1009
iprec_at_recall_0.90  	all	0.0384
iprec_at_recall_1.00  	all	0.0098
P_5                   	all	0.7440
P_10                  	all	0.7400
P_15                  	all	0.6760
P_20                  	all	0.6170
P_30                  	all	0.5153
P_100                 	all	0.2840
P_200                 	all	0.1678
P_500                 	all	0.0795
P_1000                	all	0.0429
</pre>
 <h4>SNUMEDINFO_CZ_Run.6.dat</h4>
            <pre>
runid                 	all	SNUMEDINFO6
num_q                 	all	50
num_ret               	all	49000
num_rel               	all	3195
num_rel_ret           	all	2147
map                   	all	0.3327
gm_map                	all	0.1855
Rprec                 	all	0.3605
bpref                 	all	0.4916
recip_rank            	all	0.7864
iprec_at_recall_0.00  	all	0.8453
iprec_at_recall_0.10  	all	0.7775
iprec_at_recall_0.20  	all	0.6211
iprec_at_recall_0.30  	all	0.4733
iprec_at_recall_0.40  	all	0.3860
iprec_at_recall_0.50  	all	0.2929
iprec_at_recall_0.60  	all	0.2034
iprec_at_recall_0.70  	all	0.1382
iprec_at_recall_0.80  	all	0.0882
iprec_at_recall_0.90  	all	0.0385
iprec_at_recall_1.00  	all	0.0095
P_5                   	all	0.7240
P_10                  	all	0.7320
P_15                  	all	0.6800
P_20                  	all	0.6150
P_30                  	all	0.5073
P_100                 	all	0.2746
P_200                 	all	0.1655
P_500                 	all	0.0793
P_1000                	all	0.0429
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>SNUMEDINFO_CZ_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.7128
ndcg_cut_10           	all	0.6940
ndcg_cut_15           	all	0.6684
ndcg_cut_20           	all	0.6285
ndcg_cut_30           	all	0.5662
ndcg_cut_100          	all	0.4905
ndcg_cut_200          	all	0.5301
ndcg_cut_500          	all	0.5723
ndcg_cut_1000         	all	0.6001
</pre>
 <h4>SNUMEDINFO_CZ_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6998
ndcg_cut_10           	all	0.7011
ndcg_cut_15           	all	0.6655
ndcg_cut_20           	all	0.6316
ndcg_cut_30           	all	0.5665
ndcg_cut_100          	all	0.4983
ndcg_cut_200          	all	0.5377
ndcg_cut_500          	all	0.5810
ndcg_cut_1000         	all	0.6001
</pre>
 <h4>SNUMEDINFO_CZ_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6834
ndcg_cut_10           	all	0.6871
ndcg_cut_15           	all	0.6628
ndcg_cut_20           	all	0.6271
ndcg_cut_30           	all	0.5582
ndcg_cut_100          	all	0.4853
ndcg_cut_200          	all	0.5289
ndcg_cut_500          	all	0.5750
ndcg_cut_1000         	all	0.5949
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>SNUMEDINFO_CZ_Run.1.dat</h4>
<img src="./img/SNUMEDINFO_CZ_Run.1.dat.p10.png"/>
 <h4>SNUMEDINFO_CZ_Run.5.dat</h4>
<img src="./img/SNUMEDINFO_CZ_Run.5.dat.p10.png"/>
 <h4>SNUMEDINFO_CZ_Run.6.dat</h4>
<img src="./img/SNUMEDINFO_CZ_Run.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
