<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - GRIUM_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>GRIUM_EN_Run.1.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_baseline
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	2537
map                   	all	0.3945
gm_map                	all	0.3337
Rprec                 	all	0.4201
bpref                 	all	0.5595
recip_rank            	all	0.8549
iprec_at_recall_0.00  	all	0.9130
iprec_at_recall_0.10  	all	0.7919
iprec_at_recall_0.20  	all	0.6980
iprec_at_recall_0.30  	all	0.6075
iprec_at_recall_0.40  	all	0.4789
iprec_at_recall_0.50  	all	0.3671
iprec_at_recall_0.60  	all	0.2884
iprec_at_recall_0.70  	all	0.1988
iprec_at_recall_0.80  	all	0.1210
iprec_at_recall_0.90  	all	0.0728
iprec_at_recall_1.00  	all	0.0197
P_5                   	all	0.7240
P_10                  	all	0.7180
P_15                  	all	0.6867
P_20                  	all	0.6490
P_30                  	all	0.5680
P_100                 	all	0.3106
P_200                 	all	0.1888
P_500                 	all	0.0911
P_1000                	all	0.0507
</pre>
 <h4>GRIUM_EN_Run.5.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_combineOrigSuiName
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	2550
map                   	all	0.4016
gm_map                	all	0.3464
Rprec                 	all	0.4242
bpref                 	all	0.5699
recip_rank            	all	0.8842
iprec_at_recall_0.00  	all	0.9235
iprec_at_recall_0.10  	all	0.8168
iprec_at_recall_0.20  	all	0.7344
iprec_at_recall_0.30  	all	0.6103
iprec_at_recall_0.40  	all	0.4702
iprec_at_recall_0.50  	all	0.3678
iprec_at_recall_0.60  	all	0.2940
iprec_at_recall_0.70  	all	0.2022
iprec_at_recall_0.80  	all	0.1220
iprec_at_recall_0.90  	all	0.0707
iprec_at_recall_1.00  	all	0.0204
P_5                   	all	0.7680
P_10                  	all	0.7560
P_15                  	all	0.7067
P_20                  	all	0.6370
P_30                  	all	0.5533
P_100                 	all	0.3144
P_200                 	all	0.1912
P_500                 	all	0.0924
P_1000                	all	0.0510
</pre>
 <h4>GRIUM_EN_Run.6.dat</h4>
            <pre>
runid                 	all	GRIUM_EN_mutualInfoExp
num_q                 	all	50
num_ret               	all	49349
num_rel               	all	3209
num_rel_ret           	all	2549
map                   	all	0.4007
gm_map                	all	0.3387
Rprec                 	all	0.4156
bpref                 	all	0.5584
recip_rank            	all	0.8563
iprec_at_recall_0.00  	all	0.9171
iprec_at_recall_0.10  	all	0.8077
iprec_at_recall_0.20  	all	0.7268
iprec_at_recall_0.30  	all	0.6170
iprec_at_recall_0.40  	all	0.4727
iprec_at_recall_0.50  	all	0.3727
iprec_at_recall_0.60  	all	0.2931
iprec_at_recall_0.70  	all	0.2117
iprec_at_recall_0.80  	all	0.1229
iprec_at_recall_0.90  	all	0.0735
iprec_at_recall_1.00  	all	0.0207
P_5                   	all	0.7480
P_10                  	all	0.7120
P_15                  	all	0.6933
P_20                  	all	0.6510
P_30                  	all	0.5627
P_100                 	all	0.3132
P_200                 	all	0.1921
P_500                 	all	0.0924
P_1000                	all	0.0510
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>GRIUM_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.7009
ndcg_cut_10           	all	0.7033
ndcg_cut_15           	all	0.6887
ndcg_cut_20           	all	0.6705
ndcg_cut_30           	all	0.6235
ndcg_cut_100          	all	0.5500
ndcg_cut_200          	all	0.5981
ndcg_cut_500          	all	0.6534
ndcg_cut_1000         	all	0.6847
</pre>
 <h4>GRIUM_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.7423
ndcg_cut_10           	all	0.7445
ndcg_cut_15           	all	0.7182
ndcg_cut_20           	all	0.6801
ndcg_cut_30           	all	0.6283
ndcg_cut_100          	all	0.5600
ndcg_cut_200          	all	0.6112
ndcg_cut_500          	all	0.6669
ndcg_cut_1000         	all	0.6964
</pre>
 <h4>GRIUM_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.7163
ndcg_cut_10           	all	0.7077
ndcg_cut_15           	all	0.6980
ndcg_cut_20           	all	0.6741
ndcg_cut_30           	all	0.6247
ndcg_cut_100          	all	0.5549
ndcg_cut_200          	all	0.6069
ndcg_cut_500          	all	0.6612
ndcg_cut_1000         	all	0.6902
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>GRIUM_EN_Run.1.dat</h4>
<img src="./img/GRIUM_EN_Run.1.dat.p10.png"/>
 <h4>GRIUM_EN_Run.5.dat</h4>
<img src="./img/GRIUM_EN_Run.5.dat.p10.png"/>
 <h4>GRIUM_EN_Run.6.dat</h4>
<img src="./img/GRIUM_EN_Run.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
