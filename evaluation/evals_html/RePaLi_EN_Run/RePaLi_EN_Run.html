<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - RePaLi_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>RePaLi_EN_Run.1.dat</h4>
            <pre>
runid                 	all	indri
num_q                 	all	50
num_ret               	all	49000
num_rel               	all	3132
num_rel_ret           	all	2564
map                   	all	0.3973
gm_map                	all	0.2706
Rprec                 	all	0.4062
bpref                 	all	0.5423
recip_rank            	all	0.7786
iprec_at_recall_0.00  	all	0.8464
iprec_at_recall_0.10  	all	0.7535
iprec_at_recall_0.20  	all	0.6924
iprec_at_recall_0.30  	all	0.6253
iprec_at_recall_0.40  	all	0.5025
iprec_at_recall_0.50  	all	0.3803
iprec_at_recall_0.60  	all	0.3073
iprec_at_recall_0.70  	all	0.2297
iprec_at_recall_0.80  	all	0.1489
iprec_at_recall_0.90  	all	0.0788
iprec_at_recall_1.00  	all	0.0262
P_5                   	all	0.6840
P_10                  	all	0.6480
P_15                  	all	0.6253
P_20                  	all	0.6050
P_30                  	all	0.5453
P_100                 	all	0.3202
P_200                 	all	0.1969
P_500                 	all	0.0932
P_1000                	all	0.0513
</pre>
 <h4>RePaLi_EN_Run.5.dat</h4>
            <pre>
runid                 	all	indri
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2618
map                   	all	0.4021
gm_map                	all	0.3364
Rprec                 	all	0.4165
bpref                 	all	0.5550
recip_rank            	all	0.7937
iprec_at_recall_0.00  	all	0.8587
iprec_at_recall_0.10  	all	0.7659
iprec_at_recall_0.20  	all	0.6899
iprec_at_recall_0.30  	all	0.6155
iprec_at_recall_0.40  	all	0.5115
iprec_at_recall_0.50  	all	0.4046
iprec_at_recall_0.60  	all	0.3109
iprec_at_recall_0.70  	all	0.2327
iprec_at_recall_0.80  	all	0.1474
iprec_at_recall_0.90  	all	0.0883
iprec_at_recall_1.00  	all	0.0218
P_5                   	all	0.6920
P_10                  	all	0.6740
P_15                  	all	0.6400
P_20                  	all	0.6260
P_30                  	all	0.5753
P_100                 	all	0.3252
P_200                 	all	0.2020
P_500                 	all	0.0969
P_1000                	all	0.0524
</pre>
 <h4>RePaLi_EN_Run.6.dat</h4>
            <pre>
runid                 	all	indri
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2424
map                   	all	0.3564
gm_map                	all	0.2368
Rprec                 	all	0.3813
bpref                 	all	0.5154
recip_rank            	all	0.7685
iprec_at_recall_0.00  	all	0.8305
iprec_at_recall_0.10  	all	0.7164
iprec_at_recall_0.20  	all	0.6386
iprec_at_recall_0.30  	all	0.5453
iprec_at_recall_0.40  	all	0.4272
iprec_at_recall_0.50  	all	0.3316
iprec_at_recall_0.60  	all	0.2568
iprec_at_recall_0.70  	all	0.1920
iprec_at_recall_0.80  	all	0.1259
iprec_at_recall_0.90  	all	0.0640
iprec_at_recall_1.00  	all	0.0180
P_5                   	all	0.6880
P_10                  	all	0.6600
P_15                  	all	0.6160
P_20                  	all	0.5760
P_30                  	all	0.5180
P_100                 	all	0.3006
P_200                 	all	0.1801
P_500                 	all	0.0861
P_1000                	all	0.0485
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>RePaLi_EN_Run.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6691
ndcg_cut_10           	all	0.6520
ndcg_cut_15           	all	0.6402
ndcg_cut_20           	all	0.6301
ndcg_cut_30           	all	0.5996
ndcg_cut_100          	all	0.5499
ndcg_cut_200          	all	0.6062
ndcg_cut_500          	all	0.6588
ndcg_cut_1000         	all	0.6871
</pre>
 <h4>RePaLi_EN_Run.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6927
ndcg_cut_10           	all	0.6793
ndcg_cut_15           	all	0.6593
ndcg_cut_20           	all	0.6506
ndcg_cut_30           	all	0.6260
ndcg_cut_100          	all	0.5624
ndcg_cut_200          	all	0.6223
ndcg_cut_500          	all	0.6780
ndcg_cut_1000         	all	0.7008
</pre>
 <h4>RePaLi_EN_Run.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.6749
ndcg_cut_10           	all	0.6590
ndcg_cut_15           	all	0.6337
ndcg_cut_20           	all	0.6097
ndcg_cut_30           	all	0.5755
ndcg_cut_100          	all	0.5196
ndcg_cut_200          	all	0.5649
ndcg_cut_500          	all	0.6187
ndcg_cut_1000         	all	0.6534
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>RePaLi_EN_Run.1.dat</h4>
<img src="./img/RePaLi_EN_Run.1.dat.p10.png"/>
 <h4>RePaLi_EN_Run.5.dat</h4>
<img src="./img/RePaLi_EN_Run.5.dat.p10.png"/>
 <h4>RePaLi_EN_Run.6.dat</h4>
<img src="./img/RePaLi_EN_Run.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
