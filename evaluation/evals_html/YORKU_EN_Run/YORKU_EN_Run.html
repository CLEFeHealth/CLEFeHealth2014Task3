<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - YORKU_EN_Run</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>YORKU_EN_Run.1.txt</h4>
            <pre>
runid                 	all	runid
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2296
map                   	all	0.1725
gm_map                	all	0.0944
Rprec                 	all	0.1883
bpref                 	all	0.4263
recip_rank            	all	0.6164
iprec_at_recall_0.00  	all	0.6799
iprec_at_recall_0.10  	all	0.3688
iprec_at_recall_0.20  	all	0.2716
iprec_at_recall_0.30  	all	0.2236
iprec_at_recall_0.40  	all	0.1901
iprec_at_recall_0.50  	all	0.1461
iprec_at_recall_0.60  	all	0.1162
iprec_at_recall_0.70  	all	0.0879
iprec_at_recall_0.80  	all	0.0567
iprec_at_recall_0.90  	all	0.0393
iprec_at_recall_1.00  	all	0.0085
P_5                   	all	0.4640
P_10                  	all	0.4360
P_15                  	all	0.3427
P_20                  	all	0.3020
P_30                  	all	0.2580
P_100                 	all	0.1416
P_200                 	all	0.1055
P_500                 	all	0.0652
P_1000                	all	0.0459
</pre>
 <h4>YORKU_EN_Run.5.txt</h4>
            <pre>
runid                 	all	runid
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2549
map                   	all	0.3207
gm_map                	all	0.2427
Rprec                 	all	0.3413
bpref                 	all	0.5002
recip_rank            	all	0.7212
iprec_at_recall_0.00  	all	0.7979
iprec_at_recall_0.10  	all	0.6579
iprec_at_recall_0.20  	all	0.5592
iprec_at_recall_0.30  	all	0.4548
iprec_at_recall_0.40  	all	0.3742
iprec_at_recall_0.50  	all	0.3078
iprec_at_recall_0.60  	all	0.2594
iprec_at_recall_0.70  	all	0.1798
iprec_at_recall_0.80  	all	0.1271
iprec_at_recall_0.90  	all	0.0671
iprec_at_recall_1.00  	all	0.0199
P_5                   	all	0.5840
P_10                  	all	0.6040
P_15                  	all	0.5480
P_20                  	all	0.5220
P_30                  	all	0.4540
P_100                 	all	0.2778
P_200                 	all	0.1840
P_500                 	all	0.0921
P_1000                	all	0.0510
</pre>
 <h4>YORKU_EN_Run.6.txt</h4>
            <pre>
runid                 	all	runid
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	2531
map                   	all	0.0625
gm_map                	all	0.0464
Rprec                 	all	0.0704
bpref                 	all	0.4581
recip_rank            	all	0.1344
iprec_at_recall_0.00  	all	0.1890
iprec_at_recall_0.10  	all	0.1169
iprec_at_recall_0.20  	all	0.1010
iprec_at_recall_0.30  	all	0.0931
iprec_at_recall_0.40  	all	0.0873
iprec_at_recall_0.50  	all	0.0785
iprec_at_recall_0.60  	all	0.0699
iprec_at_recall_0.70  	all	0.0570
iprec_at_recall_0.80  	all	0.0371
iprec_at_recall_0.90  	all	0.0224
iprec_at_recall_1.00  	all	0.0049
P_5                   	all	0.0640
P_10                  	all	0.0600
P_15                  	all	0.0707
P_20                  	all	0.0670
P_30                  	all	0.0627
P_100                 	all	0.0682
P_200                 	all	0.0711
P_500                 	all	0.0634
P_1000                	all	0.0506
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>YORKU_EN_Run.1.txt</h4>
            <pre>
ndcg_cut_5            	all	0.4470
ndcg_cut_10           	all	0.4305
ndcg_cut_15           	all	0.3746
ndcg_cut_20           	all	0.3456
ndcg_cut_30           	all	0.3141
ndcg_cut_100          	all	0.2755
ndcg_cut_200          	all	0.3317
ndcg_cut_500          	all	0.4118
ndcg_cut_1000         	all	0.4978
</pre>
 <h4>YORKU_EN_Run.5.txt</h4>
            <pre>
ndcg_cut_5            	all	0.5925
ndcg_cut_10           	all	0.5999
ndcg_cut_15           	all	0.5654
ndcg_cut_20           	all	0.5501
ndcg_cut_30           	all	0.5079
ndcg_cut_100          	all	0.4788
ndcg_cut_200          	all	0.5465
ndcg_cut_500          	all	0.6088
ndcg_cut_1000         	all	0.6401
</pre>
 <h4>YORKU_EN_Run.6.txt</h4>
            <pre>
ndcg_cut_5            	all	0.0566
ndcg_cut_10           	all	0.0560
ndcg_cut_15           	all	0.0646
ndcg_cut_20           	all	0.0643
ndcg_cut_30           	all	0.0632
ndcg_cut_100          	all	0.0882
ndcg_cut_200          	all	0.1535
ndcg_cut_500          	all	0.2835
ndcg_cut_1000         	all	0.4106
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>YORKU_EN_Run.1.txt</h4>
<img src="./img/YORKU_EN_Run.1.txt.p10.png"/>
 <h4>YORKU_EN_Run.5.txt</h4>
<img src="./img/YORKU_EN_Run.5.txt.p10.png"/>
 <h4>YORKU_EN_Run.6.txt</h4>
<img src="./img/YORKU_EN_Run.6.txt.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
