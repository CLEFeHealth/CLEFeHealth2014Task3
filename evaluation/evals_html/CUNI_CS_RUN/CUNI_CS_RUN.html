<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Share/CLEF eHealth 2013 TASK 3 Results by relevation - Information Retrieval Relevance Judging System</title>
   <meta name="description" content="">
    <meta name="author" content="CLEF 2013 Task 3 Committee">
    <link rel="stylesheet" href="bootstrap.css">
    <style type="text/css">
        body { padding-top: 60px; font-size: 12px; }
        label, button, .btn { font-size: 12px; }
    </style>
    <!--[if lt IE 9]>
    <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="./clef_eval_files/jquery.min.js"></script>
    <script src="./clef_eval_files/bootstrap.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="http://twitter.github.io/bootstrap/assets/css/bootstrap-responsive.css" rel="stylesheet">
<style type="text/css">@media print { #feedlyMiniIcon { display: none; } }</style></head>

<body style="zoom: 1;">
<div class="container">
<div class="well">
        <h1>Share/CLEF eHealth 2013 TASK 3 Results - CUNI_CS_RUN</h1>
        <p>This page summarises the results obtained from your submissions. Here, you can find the mean performance of your submissions, for all standard trec_eval measures and for nDCG at different ranks.</br> For this first year, the Share/CLEF eHealth 2013 TASK 3 built result pools from your submissions considering the top 10 documents ranked by your baseline system (run 1), and the highest priority run that used the discharge summaries (run 2) and the highest priority run that did not used the discharge summaries (run 5). As a consequence:<ul><li>the <b>primary measure for this year is precision at 10 (P@10)</b>,</li><li>the <b>secondary measure is Normalised Discounted Cumulative Gain at rank 10</b> (ndcg_cut_10).</li></ul>Per-query performance can be found in the <i>evals</i> folder in the directory containing this HTML page.</br>Some of the participant's runs contained formatting errors (the most common was the use of a numeric query id in place of the official qtestXX id); others did not retrieve results for some queries. To fairly evaluate the runs of every participant, submitted runs containing formatting errors have been corrected; an artificial document (fakeCLEF) has been added to the result rankings for queries where no result was retrieved. Your runs as have been evaluated are contained in the folder <i>runs</i>.</br>Please refer to the <a href="https://sites.google.com/site/shareclefehealth/">ShARe/CLEF 2013 eHealth Evaluation Lab website</a> for additional details on the task.</p>
    </div>
    <div class="row">
        <div class="span9">
            <h2>Evaluation with standard trec_eval metrics</h2>
        <p>These results have been obtained with the binary relevance assessment, i.e. qrels.clef2014.test.bin.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>. Trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 qrels.clef2014.test.bin.txt runName</pre>
 <h4>CUNI_CS_RUN.1.dat</h4>
            <pre>
runid                 	all	RUN1
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	1965
map                   	all	0.2151
gm_map                	all	0.0893
Rprec                 	all	0.2696
bpref                 	all	0.3863
recip_rank            	all	0.5054
iprec_at_recall_0.00  	all	0.6231
iprec_at_recall_0.10  	all	0.4707
iprec_at_recall_0.20  	all	0.3813
iprec_at_recall_0.30  	all	0.3126
iprec_at_recall_0.40  	all	0.2545
iprec_at_recall_0.50  	all	0.2193
iprec_at_recall_0.60  	all	0.1593
iprec_at_recall_0.70  	all	0.1102
iprec_at_recall_0.80  	all	0.0573
iprec_at_recall_0.90  	all	0.0222
iprec_at_recall_1.00  	all	0.0056
P_5                   	all	0.4400
P_10                  	all	0.4340
P_15                  	all	0.3853
P_20                  	all	0.3520
P_30                  	all	0.3193
P_100                 	all	0.2014
P_200                 	all	0.1408
P_500                 	all	0.0706
P_1000                	all	0.0393
</pre>
 <h4>CUNI_CS_RUN.5.dat</h4>
            <pre>
runid                 	all	RUN5
num_q                 	all	50
num_ret               	all	49051
num_rel               	all	3209
num_rel_ret           	all	2112
map                   	all	0.2399
gm_map                	all	0.1364
Rprec                 	all	0.2950
bpref                 	all	0.4245
recip_rank            	all	0.5736
iprec_at_recall_0.00  	all	0.6858
iprec_at_recall_0.10  	all	0.5322
iprec_at_recall_0.20  	all	0.4330
iprec_at_recall_0.30  	all	0.3414
iprec_at_recall_0.40  	all	0.2768
iprec_at_recall_0.50  	all	0.2389
iprec_at_recall_0.60  	all	0.1784
iprec_at_recall_0.70  	all	0.1268
iprec_at_recall_0.80  	all	0.0634
iprec_at_recall_0.90  	all	0.0269
iprec_at_recall_1.00  	all	0.0056
P_5                   	all	0.4920
P_10                  	all	0.4880
P_15                  	all	0.4360
P_20                  	all	0.3970
P_30                  	all	0.3553
P_100                 	all	0.2226
P_200                 	all	0.1543
P_500                 	all	0.0767
P_1000                	all	0.0422
</pre>
 <h4>CUNI_CS_RUN.6.dat</h4>
            <pre>
runid                 	all	RUN6
num_q                 	all	50
num_ret               	all	50000
num_rel               	all	3209
num_rel_ret           	all	1591
map                   	all	0.1573
gm_map                	all	0.0324
Rprec                 	all	0.1984
bpref                 	all	0.3458
recip_rank            	all	0.6078
iprec_at_recall_0.00  	all	0.6548
iprec_at_recall_0.10  	all	0.4434
iprec_at_recall_0.20  	all	0.3019
iprec_at_recall_0.30  	all	0.2099
iprec_at_recall_0.40  	all	0.1608
iprec_at_recall_0.50  	all	0.1097
iprec_at_recall_0.60  	all	0.0791
iprec_at_recall_0.70  	all	0.0489
iprec_at_recall_0.80  	all	0.0221
iprec_at_recall_0.90  	all	0.0091
iprec_at_recall_1.00  	all	0.0035
P_5                   	all	0.4680
P_10                  	all	0.4560
P_15                  	all	0.3747
P_20                  	all	0.3140
P_30                  	all	0.2513
P_100                 	all	0.1410
P_200                 	all	0.0903
P_500                 	all	0.0498
P_1000                	all	0.0318
</pre>

<h2>Evaluation with nDCG</h2>
        <p>These results have been obtained with the graded relevance assessment, i.e. qrels.clef2014.test.graded.txt, and trec_eval 9.0 <a href="http://trec.nist.gov/trec_eval/">as distributed by NIST</a>.</br>To obtain nDCG at different ranks, trec_eval was ran as follows:</p>
<pre>./trec_eval -c -M1000 -m ndcg_cut qrels.clef2014.test.graded.txt runName</pre>
 <h4>CUNI_CS_RUN.1.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4361
ndcg_cut_10           	all	0.4335
ndcg_cut_15           	all	0.4075
ndcg_cut_20           	all	0.3856
ndcg_cut_30           	all	0.3669
ndcg_cut_100          	all	0.3555
ndcg_cut_200          	all	0.4166
ndcg_cut_500          	all	0.4690
ndcg_cut_1000         	all	0.4940
</pre>
 <h4>CUNI_CS_RUN.5.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4830
ndcg_cut_10           	all	0.4810
ndcg_cut_15           	all	0.4530
ndcg_cut_20           	all	0.4284
ndcg_cut_30           	all	0.4042
ndcg_cut_100          	all	0.3912
ndcg_cut_200          	all	0.4573
ndcg_cut_500          	all	0.5122
ndcg_cut_1000         	all	0.5361
</pre>
 <h4>CUNI_CS_RUN.6.dat</h4>
            <pre>
ndcg_cut_5            	all	0.4928
ndcg_cut_10           	all	0.4746
ndcg_cut_15           	all	0.4157
ndcg_cut_20           	all	0.3724
ndcg_cut_30           	all	0.3211
ndcg_cut_100          	all	0.2799
ndcg_cut_200          	all	0.3110
ndcg_cut_500          	all	0.3605
ndcg_cut_1000         	all	0.4030
</pre>

<h2>Plots P@10</h2>
<p>The plots below compare each of your runs against the median and best performance (p@10) across all systems  submitted to CLEF for each query topic. In particular, for each query, the height of a bar represents the gain/loss of your system and the best system (for that query) over the median system. The height of a bar in then given by:</p>
<pre>grey bars:   height(q) = your_p@10(q) - median_p@10(q)
white bars:  height(q) = best_p@10(q) - median_p@10(q)</pre>

 <h4>CUNI_CS_RUN.1.dat</h4>
<img src="./img/CUNI_CS_RUN.1.dat.p10.png"/>
 <h4>CUNI_CS_RUN.5.dat</h4>
<img src="./img/CUNI_CS_RUN.5.dat.p10.png"/>
 <h4>CUNI_CS_RUN.6.dat</h4>
<img src="./img/CUNI_CS_RUN.6.dat.p10.png"/>
      </div>
    </div>

    <p></p><hr><p></p>
    <footer class="row">
        <div class="span6">
           <p><a href="https://github.com/bevankoopman/relevation">relevation</a> - Information Retrieval Relevance Judging System</p>
      </div>
    </footer>
</div> <!-- container -->
</body></html>
